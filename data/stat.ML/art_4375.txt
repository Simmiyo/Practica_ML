Proximal Gradient Method with Extrapolation and Line Search for a Class
  of Nonconvex and Nonsmooth Problems
In this paper, we consider a class of possibly nonconvex, nonsmooth and
non-Lipschitz optimization problems arising in many contemporary applications
such as machine learning, variable selection and image processing. To solve
this class of problems, we propose a proximal gradient method with
extrapolation and line search (PGels). This method is developed based on a
special potential function and successfully incorporates both extrapolation and
non-monotone line search, which are two simple and efficient accelerating
techniques for the proximal gradient method. Thanks to the line search, this
method allows more flexibilities in choosing the extrapolation parameters and
updates them adaptively at each iteration if a certain line search criterion is
not satisfied. Moreover, with proper choices of parameters, our PGels reduces
to many existing algorithms. We also show that, under some mild conditions, our
line search criterion is well defined and any cluster point of the sequence
generated by PGels is a stationary point of our problem. In addition, by
assuming the Kurdyka-${\L}$ojasiewicz exponent of the objective in our problem,
we further analyze the local convergence rate of two special cases of PGels,
including the widely used non-monotone proximal gradient method as one case.
Finally, we conduct some numerical experiments for solving the $\ell_1$
regularized logistic regression problem and the $\ell_{1\text{-}2}$ regularized
least squares problem. Our numerical results illustrate the efficiency of PGels
and show the potential advantage of combining two accelerating techniques.