A vector-contraction inequality for Rademacher complexities
The contraction inequality for Rademacher averages is extended to Lipschitz
functions with vector-valued domains, and it is also shown that in the bounding
expression the Rademacher variables can be replaced by arbitrary iid symmetric
and sub-gaussian variables. Example applications are given for multi-category
learning, K-means clustering and learning-to-learn.