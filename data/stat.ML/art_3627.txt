Sliced Wasserstein Kernels for Probability Distributions
Optimal transport distances, otherwise known as Wasserstein distances, have
recently drawn ample attention in computer vision and machine learning as a
powerful discrepancy measure for probability distributions. The recent
developments on alternative formulations of the optimal transport have allowed
for faster solutions to the problem and has revamped its practical applications
in machine learning. In this paper, we exploit the widely used kernel methods
and provide a family of provably positive definite kernels based on the Sliced
Wasserstein distance and demonstrate the benefits of these kernels in a variety
of learning tasks. Our work provides a new perspective on the application of
optimal transport flavored distances through kernel methods in machine learning
tasks.