Transfer Learning by Asymmetric Image Weighting for Segmentation across
  Scanners
Supervised learning has been very successful for automatic segmentation of
images from a single scanner. However, several papers report deteriorated
performances when using classifiers trained on images from one scanner to
segment images from other scanners. We propose a transfer learning classifier
that adapts to differences between training and test images. This method uses a
weighted ensemble of classifiers trained on individual images. The weight of
each classifier is determined by the similarity between its training image and
the test image.
  We examine three unsupervised similarity measures, which can be used in
scenarios where no labeled data from a newly introduced scanner or scanning
protocol is available. The measures are based on a divergence, a bag distance,
and on estimating the labels with a clustering procedure. These measures are
asymmetric. We study whether the asymmetry can improve classification. Out of
the three similarity measures, the bag similarity measure is the most robust
across different studies and achieves excellent results on four brain tissue
segmentation datasets and three white matter lesion segmentation datasets,
acquired at different centers and with different scanners and scanning
protocols. We show that the asymmetry can indeed be informative, and that
computing the similarity from the test image to the training images is more
appropriate than the opposite direction.