On the Statistical Efficiency of $\ell_{1,p}$ Multi-Task Learning of
  Gaussian Graphical Models
In this paper, we present $\ell_{1,p}$ multi-task structure learning for
Gaussian graphical models. We analyze the sufficient number of samples for the
correct recovery of the support union and edge signs. We also analyze the
necessary number of samples for any conceivable method by providing
information-theoretic lower bounds. We compare the statistical efficiency of
multi-task learning versus that of single-task learning. For experiments, we
use a block coordinate descent method that is provably convergent and generates
a sequence of positive definite solutions. We provide experimental validation
on synthetic data as well as on two publicly available real-world data sets,
including functional magnetic resonance imaging and gene expression data.