Resolving the Geometric Locus Dilemma for Support Vector Learning
  Machines
Capacity control, the bias/variance dilemma, and learning unknown functions
from data, are all concerned with identifying effective and consistent fits of
unknown geometric loci to random data points. A geometric locus is a curve or
surface formed by points, all of which possess some uniform property. A
geometric locus of an algebraic equation is the set of points whose coordinates
are solutions of the equation. Any given curve or surface must pass through
each point on a specified locus. This paper argues that it is impossible to fit
random data points to algebraic equations of partially configured geometric
loci that reference arbitrary Cartesian coordinate systems. It also argues that
the fundamental curve of a linear decision boundary is actually a principal
eigenaxis. It is shown that learning principal eigenaxes of linear decision
boundaries involves finding a point of statistical equilibrium for which
eigenenergies of principal eigenaxis components are symmetrically balanced with
each other. It is demonstrated that learning linear decision boundaries
involves strong duality relationships between a statistical eigenlocus of
principal eigenaxis components and its algebraic forms, in primal and dual,
correlated Hilbert spaces. Locus equations are introduced and developed that
describe principal eigen-coordinate systems for lines, planes, and hyperplanes.
These equations are used to introduce and develop primal and dual statistical
eigenlocus equations of principal eigenaxes of linear decision boundaries.
Important generalizations for linear decision boundaries are shown to be
encoded within a dual statistical eigenlocus of principal eigenaxis components.
Principal eigenaxes of linear decision boundaries are shown to encode Bayes'
likelihood ratio for common covariance data and a robust likelihood ratio for
all other data.