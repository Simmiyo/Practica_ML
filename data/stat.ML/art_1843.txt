Bayesian Active Learning for Classification and Preference Learning
Information theoretic active learning has been widely studied for
probabilistic models. For simple regression an optimal myopic policy is easily
tractable. However, for other tasks and with more complex models, such as
classification with nonparametric models, the optimal solution is harder to
compute. Current approaches make approximations to achieve tractability. We
propose an approach that expresses information gain in terms of predictive
entropies, and apply this method to the Gaussian Process Classifier (GPC). Our
approach makes minimal approximations to the full information theoretic
objective. Our experimental performance compares favourably to many popular
active learning algorithms, and has equal or lower computational complexity. We
compare well to decision theoretic approaches also, which are privy to more
information and require much more computational time. Secondly, by developing
further a reformulation of binary preference learning to a classification
problem, we extend our algorithm to Gaussian Process preference learning.