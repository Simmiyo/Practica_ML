Learning Rates for Kernel-Based Expectile Regression
Conditional expectiles are becoming an increasingly important tool in finance
as well as in other areas of applications. We analyse a support vector machine
type approach for estimating conditional expectiles and establish learning
rates that are minimax optimal modulo a logarithmic factor if Gaussian RBF
kernels are used and the desired expectile is smooth in a Besov sense. As a
special case, our learning rates improve the best known rates for kernel-based
least squares regression in this scenario. Key ingredients of our statistical
analysis are a general calibration inequality for the asymmetric least squares
loss, a corresponding variance bound as well as an improved entropy number
bound for Gaussian RBF kernels.