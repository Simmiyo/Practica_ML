Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits
We present a new algorithm for the contextual bandit learning problem, where
the learner repeatedly takes one of $K$ actions in response to the observed
context, and observes the reward only for that chosen action. Our method
assumes access to an oracle for solving fully supervised cost-sensitive
classification problems and achieves the statistically optimal regret guarantee
with only $\tilde{O}(\sqrt{KT/\log N})$ oracle calls across all $T$ rounds,
where $N$ is the number of policies in the policy class we compete against. By
doing so, we obtain the most practical contextual bandit learning algorithm
amongst approaches that work for general policy classes. We further conduct a
proof-of-concept experiment which demonstrates the excellent computational and
prediction performance of (an online variant of) our algorithm relative to
several baselines.