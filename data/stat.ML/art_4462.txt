Estimation of RÃ©nyi Entropy and Mutual Information Based on
  Generalized Nearest-Neighbor Graphs
We present simple and computationally efficient nonparametric estimators of
R\'enyi entropy and mutual information based on an i.i.d. sample drawn from an
unknown, absolutely continuous distribution over $\R^d$. The estimators are
calculated as the sum of $p$-th powers of the Euclidean lengths of the edges of
the `generalized nearest-neighbor' graph of the sample and the empirical copula
of the sample respectively. For the first time, we prove the almost sure
consistency of these estimators and upper bounds on their rates of convergence,
the latter of which under the assumption that the density underlying the sample
is Lipschitz continuous. Experiments demonstrate their usefulness in
independent subspace analysis.