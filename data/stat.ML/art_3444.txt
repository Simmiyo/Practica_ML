Communication Efficient Distributed Agnostic Boosting
We consider the problem of learning from distributed data in the agnostic
setting, i.e., in the presence of arbitrary forms of noise. Our main
contribution is a general distributed boosting-based procedure for learning an
arbitrary concept space, that is simultaneously noise tolerant, communication
efficient, and computationally efficient. This improves significantly over
prior works that were either communication efficient only in noise-free
scenarios or computationally prohibitive. Empirical results on large synthetic
and real-world datasets demonstrate the effectiveness and scalability of the
proposed approach.