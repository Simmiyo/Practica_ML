Accelerating Optimization via Adaptive Prediction
We present a powerful general framework for designing data-dependent
optimization algorithms, building upon and unifying recent techniques in
adaptive regularization, optimistic gradient predictions, and problem-dependent
randomization. We first present a series of new regret guarantees that hold at
any time and under very minimal assumptions, and then show how different
relaxations recover existing algorithms, both basic as well as more recent
sophisticated ones. Finally, we show how combining adaptivity, optimism, and
problem-dependent randomization can guide the design of algorithms that benefit
from more favorable guarantees than recent state-of-the-art methods.