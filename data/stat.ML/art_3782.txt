Fast model selection by limiting SVM training times
Kernelized Support Vector Machines (SVMs) are among the best performing
supervised learning methods. But for optimal predictive performance,
time-consuming parameter tuning is crucial, which impedes application. To
tackle this problem, the classic model selection procedure based on grid-search
and cross-validation was refined, e.g. by data subsampling and direct search
heuristics. Here we focus on a different aspect, the stopping criterion for SVM
training. We show that by limiting the training time given to the SVM solver
during parameter tuning we can reduce model selection times by an order of
magnitude.