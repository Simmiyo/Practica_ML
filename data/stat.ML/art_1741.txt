SERAPH: Semi-supervised Metric Learning Paradigm with Hyper Sparsity
We propose a general information-theoretic approach called Seraph
(SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metric
learning that does not rely upon the manifold assumption. Given the probability
parameterized by a Mahalanobis distance, we maximize the entropy of that
probability on labeled data and minimize it on unlabeled data following entropy
regularization, which allows the supervised and unsupervised parts to be
integrated in a natural and meaningful way. Furthermore, Seraph is regularized
by encouraging a low-rank projection induced from the metric. The optimization
of Seraph is solved efficiently and stably by an EM-like scheme with the
analytical E-Step and convex M-Step. Experiments demonstrate that Seraph
compares favorably with many well-known global and local metric learning
methods.