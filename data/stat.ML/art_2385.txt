Relative Loss Bounds for On-line Density Estimation with the Exponential
  Family of Distributions
We consider on-line density estimation with a parameterized density from the
exponential family. The on-line algorithm receives one example at a time and
maintains a parameter that is essentially an average of the past examples.
After receiving an example the algorithm incurs a loss which is the negative
log-likelihood of the example w.r.t. the past parameter of the algorithm. An
off-line algorithm can choose the best parameter based on all the examples. We
prove bounds on the additional total loss of the on-line algorithm over the
total loss of the off-line algorithm. These relative loss bounds hold for an
arbitrary sequence of examples. The goal is to design algorithms with the best
possible relative loss bounds. We use a certain divergence to derive and
analyze the algorithms. This divergence is a relative entropy between two
exponential distributions.