Unregularized Online Learning Algorithms with General Loss Functions
In this paper, we consider unregularized online learning algorithms in a
Reproducing Kernel Hilbert Spaces (RKHS). Firstly, we derive explicit
convergence rates of the unregularized online learning algorithms for
classification associated with a general gamma-activating loss (see Definition
1 in the paper). Our results extend and refine the results in Ying and Pontil
(2008) for the least-square loss and the recent result in Bach and Moulines
(2011) for the loss function with a Lipschitz-continuous gradient. Moreover, we
establish a very general condition on the step sizes which guarantees the
convergence of the last iterate of such algorithms. Secondly, we establish, for
the first time, the convergence of the unregularized pairwise learning
algorithm with a general loss function and derive explicit rates under the
assumption of polynomially decaying step sizes. Concrete examples are used to
illustrate our main results. The main techniques are tools from convex
analysis, refined inequalities of Gaussian averages, and an induction approach.