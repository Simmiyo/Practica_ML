Support vector machine and its bias correction in high-dimension,
  low-sample-size settings
In this paper, we consider asymptotic properties of the support vector
machine (SVM) in high-dimension, low-sample-size (HDLSS) settings. We show that
the hard-margin linear SVM holds a consistency property in which
misclassification rates tend to zero as the dimension goes to infinity under
certain severe conditions. We show that the SVM is very biased in HDLSS
settings and its performance is affected by the bias directly. In order to
overcome such difficulties, we propose a bias-corrected SVM (BC-SVM). We show
that the BC-SVM gives preferable performances in HDLSS settings. We also
discuss the SVMs in multiclass HDLSS settings. Finally, we check the
performance of the classifiers in actual data analyses.