Sequential Convolutional Neural Networks for Slot Filling in Spoken
  Language Understanding
We investigate the usage of convolutional neural networks (CNNs) for the slot
filling task in spoken language understanding. We propose a novel CNN
architecture for sequence labeling which takes into account the previous
context words with preserved order information and pays special attention to
the current word with its surrounding context. Moreover, it combines the
information from the past and the future words for classification. Our proposed
CNN architecture outperforms even the previously best ensembling recurrent
neural network model and achieves state-of-the-art results with an F1-score of
95.61% on the ATIS benchmark dataset without using any additional linguistic
knowledge and resources.