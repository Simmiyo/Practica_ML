Still not there? Comparing Traditional Sequence-to-Sequence Models to
  Encoder-Decoder Neural Networks on Monotone String Translation Tasks
We analyze the performance of encoder-decoder neural models and compare them
with well-known established methods. The latter represent different classes of
traditional approaches that are applied to the monotone sequence-to-sequence
tasks OCR post-correction, spelling correction, grapheme-to-phoneme conversion,
and lemmatization. Such tasks are of practical relevance for various
higher-level research fields including digital humanities, automatic text
correction, and speech recognition. We investigate how well generic
deep-learning approaches adapt to these tasks, and how they perform in
comparison with established and more specialized methods, including our own
adaptation of pruned CRFs.