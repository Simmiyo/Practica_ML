Toward Multilingual Neural Machine Translation with Universal Encoder
  and Decoder
In this paper, we present our first attempts in building a multilingual
Neural Machine Translation framework under a unified approach. We are then able
to employ attention-based NMT for many-to-many multilingual translation tasks.
Our approach does not require any special treatment on the network architecture
and it allows us to learn minimal number of free parameters in a standard way
of training. Our approach has shown its effectiveness in an under-resourced
translation scenario with considerable improvements up to 2.6 BLEU points. In
addition, the approach has achieved interesting and promising results when
applied in the translation task that there is no direct parallel corpus between
source and target languages.