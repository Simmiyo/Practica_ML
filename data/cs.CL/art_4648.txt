Unsupervised Sentence Compression using Denoising Auto-Encoders
In sentence compression, the task of shortening sentences while retaining the
original meaning, models tend to be trained on large corpora containing pairs
of verbose and compressed sentences. To remove the need for paired corpora, we
emulate a summarization task and add noise to extend sentences and train a
denoising auto-encoder to recover the original, constructing an end-to-end
training regime without the need for any examples of compressed sentences. We
conduct a human evaluation of our model on a standard text summarization
dataset and show that it performs comparably to a supervised baseline based on
grammatical correctness and retention of meaning. Despite being exposed to no
target data, our unsupervised models learn to generate imperfect but reasonably
readable sentence summaries. Although we underperform supervised models based
on ROUGE scores, our models are competitive with a supervised baseline based on
human evaluation for grammatical correctness and retention of meaning.