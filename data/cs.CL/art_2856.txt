A Comparison of Neural Models for Word Ordering
We compare several language models for the word-ordering task and propose a
new bag-to-sequence neural model based on attention-based sequence-to-sequence
models. We evaluate the model on a large German WMT data set where it
significantly outperforms existing models. We also describe a novel search
strategy for LM-based word ordering and report results on the English Penn
Treebank. Our best model setup outperforms prior work both in terms of speed
and quality.