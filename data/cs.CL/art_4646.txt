Multitask and Multilingual Modelling for Lexical Analysis
In Natural Language Processing (NLP), one traditionally considers a single
task (e.g. part-of-speech tagging) for a single language (e.g. English) at a
time. However, recent work has shown that it can be beneficial to take
advantage of relatedness between tasks, as well as between languages. In this
work I examine the concept of relatedness and explore how it can be utilised to
build NLP models that require less manually annotated data. A large selection
of NLP tasks is investigated for a substantial language sample comprising 60
languages. The results show potential for joint multitask and multilingual
modelling, and hints at linguistic insights which can be gained from such
models.