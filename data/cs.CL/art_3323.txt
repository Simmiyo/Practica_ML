Attentive Recurrent Tensor Model for Community Question Answering
A major challenge to the problem of community question answering is the
lexical and semantic gap between the sentence representations. Some solutions
to minimize this gap includes the introduction of extra parameters to deep
models or augmenting the external handcrafted features. In this paper, we
propose a novel attentive recurrent tensor network for solving the lexical and
semantic gap in community question answering. We introduce token-level and
phrase-level attention strategy that maps input sequences to the output using
trainable parameters. Further, we use the tensor parameters to introduce a
3-way interaction between question, answer and external features in vector
space. We introduce simplified tensor matrices with L2 regularization that
results in smooth optimization during training. The proposed model achieves
state-of-the-art performance on the task of answer sentence selection (TrecQA
and WikiQA datasets) while outperforming the current state-of-the-art on the
tasks of best answer selection (Yahoo! L4) and answer triggering task (WikiQA).