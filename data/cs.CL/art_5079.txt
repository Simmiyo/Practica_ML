Input Combination Strategies for Multi-Source Transformer Decoder
In multi-source sequence-to-sequence tasks, the attention mechanism can be
modeled in several ways. This topic has been thoroughly studied on recurrent
architectures. In this paper, we extend the previous work to the
encoder-decoder attention in the Transformer architecture. We propose four
different input combination strategies for the encoder-decoder attention:
serial, parallel, flat, and hierarchical. We evaluate our methods on tasks of
multimodal translation and translation with multiple source languages. The
experiments show that the models are able to use multiple sources and improve
over single source baselines.