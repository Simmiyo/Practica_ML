Statistical Model Compression for Small-Footprint Natural Language
  Understanding
In this paper we investigate statistical model compression applied to natural
language understanding (NLU) models. Small-footprint NLU models are important
for enabling offline systems on hardware restricted devices, and for decreasing
on-demand model loading latency in cloud-based systems. To compress NLU models,
we present two main techniques, parameter quantization and perfect feature
hashing. These techniques are complementary to existing model pruning
strategies such as L1 regularization. We performed experiments on a large scale
NLU system. The results show that our approach achieves 14-fold reduction in
memory usage compared to the original models with minimal predictive
performance impact.