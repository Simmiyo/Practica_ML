Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems
Automatic machine learning systems can inadvertently accentuate and
perpetuate inappropriate human biases. Past work on examining inappropriate
biases has largely focused on just individual systems. Further, there is no
benchmark dataset for examining inappropriate biases in systems. Here for the
first time, we present the Equity Evaluation Corpus (EEC), which consists of
8,640 English sentences carefully chosen to tease out biases towards certain
races and genders. We use the dataset to examine 219 automatic sentiment
analysis systems that took part in a recent shared task, SemEval-2018 Task 1
'Affect in Tweets'. We find that several of the systems show statistically
significant bias; that is, they consistently provide slightly higher sentiment
intensity predictions for one race or one gender. We make the EEC freely
available.