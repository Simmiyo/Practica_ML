Marian: Cost-effective High-Quality Neural Machine Translation in C++
This paper describes the submissions of the "Marian" team to the WNMT 2018
shared task. We investigate combinations of teacher-student training,
low-precision matrix products, auto-tuning and other methods to optimize the
Transformer model on GPU and CPU. By further integrating these methods with the
new averaging attention networks, a recently introduced faster Transformer
variant, we create a number of high-quality, high-performance models on the GPU
and CPU, dominating the Pareto frontier for this shared task.