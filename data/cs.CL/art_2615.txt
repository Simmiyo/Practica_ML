Analysing Data-To-Text Generation Benchmarks
Recently, several data-sets associating data to text have been created to
train data-to-text surface realisers. It is unclear however to what extent the
surface realisation task exercised by these data-sets is linguistically
challenging. Do these data-sets provide enough variety to encourage the
development of generic, high-quality data-to-text surface realisers ? In this
paper, we argue that these data-sets have important drawbacks. We back up our
claim using statistics, metrics and manual evaluation. We conclude by eliciting
a set of criteria for the creation of a data-to-text benchmark which could help
better support the development, evaluation and comparison of linguistically
sophisticated data-to-text surface realisers.