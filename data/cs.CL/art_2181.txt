Multilingual Word Embeddings using Multigraphs
We present a family of neural-network--inspired models for computing
continuous word representations, specifically designed to exploit both
monolingual and multilingual text. This framework allows us to perform
unsupervised training of embeddings that exhibit higher accuracy on syntactic
and semantic compositionality, as well as multilingual semantic similarity,
compared to previous models trained in an unsupervised fashion. We also show
that such multilingual embeddings, optimized for semantic similarity, can
improve the performance of statistical machine translation with respect to how
it handles words not present in the parallel data.