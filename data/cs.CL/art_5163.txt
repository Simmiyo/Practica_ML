Impact of Sentiment Detection to Recognize Toxic and Subversive Online
  Comments
The presence of toxic content has become a major problem for many online
communities. Moderators try to limit this problem by implementing more and more
refined comment filters, but toxic users are constantly finding new ways to
circumvent them. Our hypothesis is that while modifying toxic content and
keywords to fool filters can be easy, hiding sentiment is harder. In this
paper, we explore various aspects of sentiment detection and their correlation
to toxicity, and use our results to implement a toxicity detection tool. We
then test how adding the sentiment information helps detect toxicity in three
different real-world datasets, and incorporate subversion to these datasets to
simulate a user trying to circumvent the system. Our results show sentiment
information has a positive impact on toxicity detection against a subversive
user.