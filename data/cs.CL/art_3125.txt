Natural Language Inference from Multiple Premises
We define a novel textual entailment task that requires inference over
multiple premise sentences. We present a new dataset for this task that
minimizes trivial lexical inferences, emphasizes knowledge of everyday events,
and presents a more challenging setting for textual entailment. We evaluate
several strong neural baselines and analyze how the multiple premise task
differs from standard textual entailment.