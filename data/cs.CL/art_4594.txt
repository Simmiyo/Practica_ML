Future-Prediction-Based Model for Neural Machine Translation
We propose a novel model for Neural Machine Translation (NMT). Different from
the conventional method, our model can predict the future text length and words
at each decoding time step so that the generation can be helped with the
information from the future prediction. With such information, the model does
not stop generation without having translated enough content. Experimental
results demonstrate that our model can significantly outperform the baseline
models. Besides, our analysis reflects that our model is effective in the
prediction of the length and words of the untranslated content.