Neural Machine Translation of Logographic Languages Using Sub-character
  Level Information
Recent neural machine translation (NMT) systems have been greatly improved by
encoder-decoder models with attention mechanisms and sub-word units. However,
important differences between languages with logographic and alphabetic writing
systems have long been overlooked. This study focuses on these differences and
uses a simple approach to improve the performance of NMT systems utilizing
decomposed sub-character level information for logographic languages. Our
results indicate that our approach not only improves the translation
capabilities of NMT systems between Chinese and English, but also further
improves NMT systems between Chinese and Japanese, because it utilizes the
shared information brought by similar sub-character units.