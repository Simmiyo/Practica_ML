Upping the Ante: Towards a Better Benchmark for Chinese-to-English
  Machine Translation
There are many machine translation (MT) papers that propose novel approaches
and show improvements over their self-defined baselines. The experimental
setting in each paper often differs from one another. As such, it is hard to
determine if a proposed approach is really useful and advances the state of the
art. Chinese-to-English translation is a common translation direction in MT
papers, although there is not one widely accepted experimental setting in
Chinese-to-English MT. Our goal in this paper is to propose a benchmark in
evaluation setup for Chinese-to-English machine translation, such that the
effectiveness of a new proposed MT approach can be directly compared to
previous approaches. Towards this end, we also built a highly competitive
state-of-the-art MT system trained on a large-scale training set. Our system
outperforms reported results on NIST OpenMT test sets in almost all papers
published in major conferences and journals in computational linguistics and
artificial intelligence in the past 11 years. We argue that a standardized
benchmark on data and performance is important for meaningful comparison.