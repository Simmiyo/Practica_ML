Semantic Refinement GRU-based Neural Language Generation for Spoken
  Dialogue Systems
Natural language generation (NLG) plays a critical role in spoken dialogue
systems. This paper presents a new approach to NLG by using recurrent neural
networks (RNN), in which a gating mechanism is applied before RNN computation.
This allows the proposed model to generate appropriate sentences. The RNN-based
generator can be learned from unaligned data by jointly training sentence
planning and surface realization to produce natural language responses. The
model was extensively evaluated on four different NLG domains. The results show
that the proposed generator achieved better performance on all the NLG domains
compared to previous generators.