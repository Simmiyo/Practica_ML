Learning Multilingual Topics from Incomparable Corpus
Multilingual topic models enable crosslingual tasks by extracting consistent
topics from multilingual corpora. Most models require parallel or comparable
training corpora, which limits their ability to generalize. In this paper, we
first demystify the knowledge transfer mechanism behind multilingual topic
models by defining an alternative but equivalent formulation. Based on this
analysis, we then relax the assumption of training data required by most
existing models, creating a model that only requires a dictionary for training.
Experiments show that our new method effectively learns coherent multilingual
topics from partially and fully incomparable corpora with limited amounts of
dictionary resources.