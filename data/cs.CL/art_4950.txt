Exploring Neural Methods for Parsing Discourse Representation Structures
Neural methods have had several recent successes in semantic parsing, though
they have yet to face the challenge of producing meaning representations based
on formal semantics. We present a sequence-to-sequence neural semantic parser
that is able to produce Discourse Representation Structures (DRSs) for English
sentences with high accuracy, outperforming traditional DRS parsers. To
facilitate the learning of the output, we represent DRSs as a sequence of flat
clauses and introduce a method to verify that produced DRSs are well-formed and
interpretable. We compare models using characters and words as input and see
(somewhat surprisingly) that the former performs better than the latter. We
show that eliminating variable names from the output using De Bruijn-indices
increases parser performance. Adding silver training data boosts performance
even further.