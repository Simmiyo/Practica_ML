PSDVec: a Toolbox for Incremental and Scalable Word Embedding
PSDVec is a Python/Perl toolbox that learns word embeddings, i.e. the mapping
of words in a natural language to continuous vectors which encode the
semantic/syntactic regularities between the words. PSDVec implements a word
embedding learning method based on a weighted low-rank positive semidefinite
approximation. To scale up the learning process, we implement a blockwise
online learning algorithm to learn the embeddings incrementally. This strategy
greatly reduces the learning time of word embeddings on a large vocabulary, and
can learn the embeddings of new words without re-learning the whole vocabulary.
On 9 word similarity/analogy benchmark sets and 2 Natural Language Processing
(NLP) tasks, PSDVec produces embeddings that has the best average performance
among popular word embedding tools. PSDVec provides a new option for NLP
practitioners.