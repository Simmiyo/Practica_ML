Nonsymbolic Text Representation
We introduce the first generic text representation model that is completely
nonsymbolic, i.e., it does not require the availability of a segmentation or
tokenization method that attempts to identify words or other symbolic units in
text. This applies to training the parameters of the model on a training corpus
as well as to applying it when computing the representation of a new text. We
show that our model performs better than prior work on an information
extraction and a text denoising task.