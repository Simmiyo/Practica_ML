Encoding Prior Knowledge with Eigenword Embeddings
Canonical correlation analysis (CCA) is a method for reducing the dimension
of data represented using two views. It has been previously used to derive word
embeddings, where one view indicates a word, and the other view indicates its
context. We describe a way to incorporate prior knowledge into CCA, give a
theoretical justification for it, and test it by deriving word embeddings and
evaluating them on a myriad of datasets.