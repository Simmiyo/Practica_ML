Lexical Normalisation of Twitter Data
Twitter with over 500 million users globally, generates over 100,000 tweets
per minute . The 140 character limit per tweet, perhaps unintentionally,
encourages users to use shorthand notations and to strip spellings to their
bare minimum "syllables" or elisions e.g. "srsly". The analysis of twitter
messages which typically contain misspellings, elisions, and grammatical
errors, poses a challenge to established Natural Language Processing (NLP)
tools which are generally designed with the assumption that the data conforms
to the basic grammatical structure commonly used in English language. In order
to make sense of Twitter messages it is necessary to first transform them into
a canonical form, consistent with the dictionary or grammar. This process,
performed at the level of individual tokens ("words"), is called lexical
normalisation. This paper investigates various techniques for lexical
normalisation of Twitter data and presents the findings as the techniques are
applied to process raw data from Twitter.