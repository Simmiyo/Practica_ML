One Sentence One Model for Neural Machine Translation
Neural machine translation (NMT) becomes a new state-of-the-art and achieves
promising translation results using a simple encoder-decoder neural network.
This neural network is trained once on the parallel corpus and the fixed
network is used to translate all the test sentences. We argue that the general
fixed network cannot best fit the specific test sentences. In this paper, we
propose the dynamic NMT which learns a general network as usual, and then
fine-tunes the network for each test sentence. The fine-tune work is done on a
small set of the bilingual training data that is obtained through similarity
search according to the test sentence. Extensive experiments demonstrate that
this method can significantly improve the translation performance, especially
when highly similar sentences are available.