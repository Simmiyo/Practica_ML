Learning to Write with Cooperative Discriminators
Recurrent Neural Networks (RNNs) are powerful autoregressive sequence models,
but when used to generate natural language their output tends to be overly
generic, repetitive, and self-contradictory. We postulate that the objective
function optimized by RNN language models, which amounts to the overall
perplexity of a text, is not expressive enough to capture the notion of
communicative goals described by linguistic principles such as Grice's Maxims.
We propose learning a mixture of multiple discriminative models that can be
used to complement the RNN generator and guide the decoding process. Human
evaluation demonstrates that text generated by our system is preferred over
that of baselines by a large margin and significantly enhances the overall
coherence, style, and information content of the generated text.