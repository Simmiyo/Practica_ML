Multilingual Neural Machine Translation with Task-Specific Attention
Multilingual machine translation addresses the task of translating between
multiple source and target languages. We propose task-specific attention
models, a simple but effective technique for improving the quality of
sequence-to-sequence neural multilingual translation. Our approach seeks to
retain as much of the parameter sharing generalization of NMT models as
possible, while still allowing for language-specific specialization of the
attention model to a particular language-pair or task. Our experiments on four
languages of the Europarl corpus show that using a target-specific model of
attention provides consistent gains in translation quality for all possible
translation directions, compared to a model in which all parameters are shared.
We observe improved translation quality even in the (extreme) low-resource
zero-shot translation directions for which the model never saw explicitly
paired parallel data.