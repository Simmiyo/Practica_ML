Open Information Extraction on Scientific Text: An Evaluation
Open Information Extraction (OIE) is the task of the unsupervised creation of
structured information from text. OIE is often used as a starting point for a
number of downstream tasks including knowledge base construction, relation
extraction, and question answering. While OIE methods are targeted at being
domain independent, they have been evaluated primarily on newspaper,
encyclopedic or general web text. In this article, we evaluate the performance
of OIE on scientific texts originating from 10 different disciplines. To do so,
we use two state-of-the-art OIE systems applying a crowd-sourcing approach. We
find that OIE systems perform significantly worse on scientific text than
encyclopedic text. We also provide an error analysis and suggest areas of work
to reduce errors. Our corpus of sentences and judgments are made available.