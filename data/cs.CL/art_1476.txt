Encoder-Decoder Shift-Reduce Syntactic Parsing
Starting from NMT, encoder-decoder neu- ral networks have been used for many
NLP problems. Graph-based models and transition-based models borrowing the en-
coder components achieve state-of-the-art performance on dependency parsing and
constituent parsing, respectively. How- ever, there has not been work
empirically studying the encoder-decoder neural net- works for transition-based
parsing. We apply a simple encoder-decoder to this end, achieving comparable
results to the parser of Dyer et al. (2015) on standard de- pendency parsing,
and outperforming the parser of Vinyals et al. (2015) on con- stituent parsing.