A Comparison of Word Embeddings for English and Cross-Lingual Chinese
  Word Sense Disambiguation
Word embeddings are now ubiquitous forms of word representation in natural
language processing. There have been applications of word embeddings for
monolingual word sense disambiguation (WSD) in English, but few comparisons
have been done. This paper attempts to bridge that gap by examining popular
embeddings for the task of monolingual English WSD. Our simplified method leads
to comparable state-of-the-art performance without expensive retraining.
Cross-Lingual WSD - where the word senses of a word in a source language e come
from a separate target translation language f - can also assist in language
learning; for example, when providing translations of target vocabulary for
learners. Thus we have also applied word embeddings to the novel task of
cross-lingual WSD for Chinese and provide a public dataset for further
benchmarking. We have also experimented with using word embeddings for LSTM
networks and found surprisingly that a basic LSTM network does not work well.
We discuss the ramifications of this outcome.