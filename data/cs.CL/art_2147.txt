Shift-Reduce Constituent Parsing with Neural Lookahead Features
Transition-based models can be fast and accurate for constituent parsing.
Compared with chart-based models, they leverage richer features by extracting
history information from a parser stack, which spans over non-local
constituents. On the other hand, during incremental parsing, constituent
information on the right hand side of the current word is not utilized, which
is a relative weakness of shift-reduce parsing. To address this limitation, we
leverage a fast neural model to extract lookahead features. In particular, we
build a bidirectional LSTM model, which leverages the full sentence information
to predict the hierarchy of constituents that each word starts and ends. The
results are then passed to a strong transition-based constituent parser as
lookahead features. The resulting parser gives 1.3% absolute improvement in WSJ
and 2.3% in CTB compared to the baseline, given the highest reported accuracies
for fully-supervised parsing.