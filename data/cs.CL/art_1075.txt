Learning in the Rational Speech Acts Model
The Rational Speech Acts (RSA) model treats language use as a recursive
process in which probabilistic speaker and listener agents reason about each
other's intentions to enrich the literal semantics of their language along
broadly Gricean lines. RSA has been shown to capture many kinds of
conversational implicature, but it has been criticized as an unrealistic model
of speakers, and it has so far required the manual specification of a semantic
lexicon, preventing its use in natural language processing applications that
learn lexical knowledge from data. We address these concerns by showing how to
define and optimize a trained statistical classifier that uses the intermediate
agents of RSA as hidden layers of representation forming a non-linear
activation function. This treatment opens up new application domains and new
possibilities for learning effectively from data. We validate the model on a
referential expression generation task, showing that the best performance is
achieved by incorporating features approximating well-established insights
about natural language generation into RSA.