Massively Parallel Cross-Lingual Learning in Low-Resource Target
  Language Translation
We work on translation from rich-resource languages to low-resource
languages. The main challenges we identify are the lack of low-resource
language data, effective methods for cross-lingual transfer, and the
variable-binding problem that is common in neural systems. We build a
translation system that addresses these challenges using eight European
language families as our test ground. Firstly, we add the source and the target
family labels and study intra-family and inter-family influences for effective
cross-lingual transfer. We achieve an improvement of +9.9 in BLEU score for
English-Swedish translation using eight families compared to the single-family
multi-source multi-target baseline. Moreover, we find that training on two
neighboring families closest to the low-resource language is often enough.
Secondly, we construct an ablation study and find that reasonably good results
can be achieved even with considerably less target data. Thirdly, we address
the variable-binding problem by building an order-preserving named entity
translation model. We obtain 60.6% accuracy in qualitative evaluation where our
translations are akin to human translations in a preliminary study.