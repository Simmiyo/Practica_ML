Efficient human-like semantic representations via the Information
  Bottleneck principle
Maintaining efficient semantic representations of the environment is a major
challenge both for humans and for machines. While human languages represent
useful solutions to this problem, it is not yet clear what computational
principle could give rise to similar solutions in machines. In this work we
propose an answer to this open question. We suggest that languages compress
percepts into words by optimizing the Information Bottleneck (IB) tradeoff
between the complexity and accuracy of their lexicons. We present empirical
evidence that this principle may give rise to human-like semantic
representations, by exploring how human languages categorize colors. We show
that color naming systems across languages are near-optimal in the IB sense,
and that these natural systems are similar to artificial IB color naming
systems with a single tradeoff parameter controlling the cross-language
variability. In addition, the IB systems evolve through a sequence of
structural phase transitions, demonstrating a possible adaptation process. This
work thus identifies a computational principle that characterizes human
semantic systems, and that could usefully inform semantic representations in
machines.