Harmonic Grammar, Optimality Theory, and Syntax Learnability: An
  Empirical Exploration of Czech Word Order
This work presents a systematic theoretical and empirical comparison of the
major algorithms that have been proposed for learning Harmonic and Optimality
Theory grammars (HG and OT, respectively). By comparing learning algorithms, we
are also able to compare the closely related OT and HG frameworks themselves.
Experimental results show that the additional expressivity of the HG framework
over OT affords performance gains in the task of predicting the surface word
order of Czech sentences. We compare the perceptron with the classic Gradual
Learning Algorithm (GLA), which learns OT grammars, as well as the popular
Maximum Entropy model. In addition to showing that the perceptron is
theoretically appealing, our work shows that the performance of the HG model it
learns approaches that of the upper bound in prediction accuracy on a held out
test set and that it is capable of accurately modeling observed variation.