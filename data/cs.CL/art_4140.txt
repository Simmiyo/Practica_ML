A Comparison of Transformer and Recurrent Neural Networks on
  Multilingual Neural Machine Translation
Recently, neural machine translation (NMT) has been extended to
multilinguality, that is to handle more than one translation direction with a
single system. Multilingual NMT showed competitive performance against pure
bilingual systems. Notably, in low-resource settings, it proved to work
effectively and efficiently, thanks to shared representation space that is
forced across languages and induces a sort of transfer-learning. Furthermore,
multilingual NMT enables so-called zero-shot inference across language pairs
never seen at training time. Despite the increasing interest in this framework,
an in-depth analysis of what a multilingual NMT model is capable of and what it
is not is still missing. Motivated by this, our work (i) provides a
quantitative and comparative analysis of the translations produced by
bilingual, multilingual and zero-shot systems; (ii) investigates the
translation quality of two of the currently dominant neural architectures in
MT, which are the Recurrent and the Transformer ones; and (iii) quantitatively
explores how the closeness between languages influences the zero-shot
translation. Our analysis leverages multiple professional post-edits of
automatic translations by several different systems and focuses both on
automatic standard metrics (BLEU and TER) and on widely used error categories,
which are lexical, morphology, and word order errors.