Attending to All Mention Pairs for Full Abstract Biological Relation
  Extraction
Most work in relation extraction forms a prediction by looking at a short
span of text within a single sentence containing a single entity pair mention.
However, many relation types, particularly in biomedical text, are expressed
across sentences or require a large context to disambiguate. We propose a model
to consider all mention and entity pairs simultaneously in order to make a
prediction. We encode full paper abstracts using an efficient self-attention
encoder and form pairwise predictions between all mentions with a bi-affine
operation. An entity-pair wise pooling aggregates mention pair scores to make a
final prediction while alleviating training noise by performing within document
multi-instance learning. We improve our model's performance by jointly training
the model to predict named entities and adding an additional corpus of weakly
labeled data. We demonstrate our model's effectiveness by achieving the state
of the art on the Biocreative V Chemical Disease Relation dataset for models
without KB resources, outperforming ensembles of models which use hand-crafted
features and additional linguistic resources.