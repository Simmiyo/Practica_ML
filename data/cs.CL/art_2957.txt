Variational Inference for Logical Inference
Functional Distributional Semantics is a framework that aims to learn, from
text, semantic representations which can be interpreted in terms of truth. Here
we make two contributions to this framework. The first is to show how a type of
logical inference can be performed by evaluating conditional probabilities. The
second is to make these calculations tractable by means of a variational
approximation. This approximation also enables faster convergence during
training, allowing us to close the gap with state-of-the-art vector space
models when evaluating on semantic similarity. We demonstrate promising
performance on two tasks.