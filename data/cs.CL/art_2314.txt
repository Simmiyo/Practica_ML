Learning Concept Embeddings for Efficient Bag-of-Concepts Densification
Explicit concept space models have proven efficacy for text representation in
many natural language and text mining applications. The idea is to embed
textual structures into a semantic space of concepts which captures the main
ideas, objects, and the characteristics of these structures. The so called Bag
of Concepts (BoC) representation suffers from data sparsity causing low
similarity scores between similar texts due to low concept overlap. To address
this problem, we propose two neural embedding models to learn continuous
concept vectors. Once they are learned, we propose an efficient vector
aggregation method to generate fully continuous BoC representations. We
evaluate our concept embedding models on three tasks: 1) measuring entity
semantic relatedness and ranking where we achieve 1.6% improvement in
correlation scores, 2) dataless concept categorization where we achieve
state-of-the-art performance and reduce the categorization error rate by more
than 5% compared to five prior word and entity embedding models, and 3)
dataless document classification where our models outperform the sparse BoC
representations. In addition, by exploiting our efficient linear time vector
aggregation method, we achieve better accuracy scores with much less concept
dimensions compared to previous BoC densification methods which operate in
polynomial time and require hundreds of dimensions in the BoC representation.