Semantic Composition via Probabilistic Model Theory
Semantic composition remains an open problem for vector space models of
semantics. In this paper, we explain how the probabilistic graphical model used
in the framework of Functional Distributional Semantics can be interpreted as a
probabilistic version of model theory. Building on this, we explain how various
semantic phenomena can be recast in terms of conditional probabilities in the
graphical model. This connection between formal semantics and machine learning
is helpful in both directions: it gives us an explicit mechanism for modelling
context-dependent meanings (a challenge for formal semantics), and also gives
us well-motivated techniques for composing distributed representations (a
challenge for distributional semantics). We present results on two datasets
that go beyond word similarity, showing how these semantically-motivated
techniques improve on the performance of vector models.