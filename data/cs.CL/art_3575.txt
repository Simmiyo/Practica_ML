Attentive Sequence-to-Sequence Learning for Diacritic Restoration of
  Yorùbá Language Text
Yor\`ub\'a is a widely spoken West African language with a writing system
rich in tonal and orthographic diacritics. With very few exceptions, diacritics
are omitted from electronic texts, due to limited device and application
support. Diacritics provide morphological information, are crucial for lexical
disambiguation, pronunciation and are vital for any Yor\`ub\'a text-to-speech
(TTS), automatic speech recognition (ASR) and natural language processing (NLP)
tasks. Reframing Automatic Diacritic Restoration (ADR) as a machine translation
task, we experiment with two different attentive Sequence-to-Sequence neural
models to process undiacritized text. On our evaluation dataset, this approach
produces diacritization error rates of less than 5%. We have released
pre-trained models, datasets and source-code as an open-source project to
advance efforts on Yor\`ub\'a language technology.