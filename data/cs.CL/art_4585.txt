Contextual Encoding for Translation Quality Estimation
The task of word-level quality estimation (QE) consists of taking a source
sentence and machine-generated translation, and predicting which words in the
output are correct and which are wrong.
  In this paper, propose a method to effectively encode the local and global
contextual information for each target word using a three-part neural network
approach.
  The first part uses an embedding layer to represent words and their
part-of-speech tags in both languages. The second part leverages a
one-dimensional convolution layer to integrate local context information for
each target word. The third part applies a stack of feed-forward and recurrent
neural networks to further encode the global context in the sentence before
making the predictions. This model was submitted as the CMU entry to the
WMT2018 shared task on QE, and achieves strong results, ranking first in three
of the six tracks.