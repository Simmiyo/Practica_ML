Debugging Neural Machine Translations
In this paper, we describe a tool for debugging the output and attention
weights of neural machine translation (NMT) systems and for improved
estimations of confidence about the output based on the attention. The purpose
of the tool is to help researchers and developers find weak and faulty example
translations that their NMT systems produce without the need for reference
translations. Our tool also includes an option to directly compare translation
outputs from two different NMT engines or experiments. In addition, we present
a demo website of our tool with examples of good and bad translations:
http://attention.lielakeda.lv