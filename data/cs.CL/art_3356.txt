Emerging Language Spaces Learned From Massively Multilingual Corpora
Translations capture important information about languages that can be used
as implicit supervision in learning linguistic properties and semantic
representations. In an information-centric view, translated texts may be
considered as semantic mirrors of the original text and the significant
variations that we can observe across various languages can be used to
disambiguate a given expression using the linguistic signal that is grounded in
translation. Parallel corpora consisting of massive amounts of human
translations with a large linguistic variation can be applied to increase
abstractions and we propose the use of highly multilingual machine translation
models to find language-independent meaning representations. Our initial
experiments show that neural machine translation models can indeed learn in
such a setup and we can show that the learning algorithm picks up information
about the relation between languages in order to optimize transfer leaning with
shared parameters. The model creates a continuous language space that
represents relationships in terms of geometric distances, which we can
visualize to illustrate how languages cluster according to language families
and groups. Does this open the door for new ideas of data-driven language
typology with promising models and techniques in empirical cross-linguistic
research?