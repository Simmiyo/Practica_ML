Exploring the importance of context and embeddings in neural NER models
  for task-oriented dialogue systems
Named Entity Recognition (NER), a classic sequence labelling task, is an
essential component of natural language understanding (NLU) systems in
task-oriented dialog systems for slot filling. For well over a decade,
different methods from lookup using gazetteers and domain ontology, classifiers
over handcrafted features to end-to-end systems involving neural network
architectures have been evaluated mostly in language-independent
non-conversational settings. In this paper, we evaluate a modified version of
the recent state of the art neural architecture in a conversational setting
where messages are often short and noisy. We perform an array of experiments
with different combinations of including the previous utterance in the dialogue
as a source of additional features and using word and character level
embeddings trained on a larger external corpus. All methods are evaluated on a
combined dataset formed from two public English task-oriented conversational
datasets belonging to travel and restaurant domains respectively. For
additional evaluation, we also repeat some of our experiments after adding
automatically translated and transliterated (from translated) versions to the
English only dataset.