Getting Reliable Annotations for Sarcasm in Online Dialogues
The language used in online forums differs in many ways from that of
traditional language resources such as news. One difference is the use and
frequency of nonliteral, subjective dialogue acts such as sarcasm. Whether the
aim is to develop a theory of sarcasm in dialogue, or engineer automatic
methods for reliably detecting sarcasm, a major challenge is simply the
difficulty of getting enough reliably labelled examples. In this paper we
describe our work on methods for achieving highly reliable sarcasm annotations
from untrained annotators on Mechanical Turk. We explore the use of a number of
common statistical reliability measures, such as Kappa, Karger's, Majority
Class, and EM. We show that more sophisticated measures do not appear to yield
better results for our data than simple measures such as assuming that the
correct label is the one that a majority of Turkers apply.