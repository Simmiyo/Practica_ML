LSTM based Conversation Models
In this paper, we present a conversational model that incorporates both
context and participant role for two-party conversations. Different
architectures are explored for integrating participant role and context
information into a Long Short-term Memory (LSTM) language model. The
conversational model can function as a language model or a language generation
model. Experiments on the Ubuntu Dialog Corpus show that our model can capture
multiple turn interaction between participants. The proposed method outperforms
a traditional LSTM model as measured by language model perplexity and response
ranking. Generated responses show characteristic differences between the two
participant roles.