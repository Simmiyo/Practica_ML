Finite size effects in the correlation structure of stochastic neural
  networks: analysis of different connectivity matrices and failure of the
  mean-field theory
We quantify the finite size effects in a stochastic network made up of rate
neurons, for several kinds of recurrent connectivity matrices. This analysis is
performed by means of a perturbative expansion of the neural equations, where
the perturbative parameters are the intensities of the sources of randomness in
the system. In detail, these parameters are the variances of the background or
input noise, of the initial conditions and of the distribution of the synaptic
weights. The technique developed in this article can be used to study systems
which are invariant under the exchange of the neural indices and it allows us
to quantify the correlation structure of the network, in terms of pairwise and
higher order correlations between the neurons. We also determine the relation
between the correlation and the external input of the network, showing that
strong signals coming from the environment reduce significantly the amount of
correlation between the neurons. Moreover we prove that in general the
phenomenon of propagation of chaos does not occur, even in the thermodynamic
limit, due to the correlation structure of the 3 sources of randomness
considered in the model. Furthermore, we show that the propagation of chaos
does not depend only on the number of neurons in the network, but also and
mainly on the number of incoming connections per neuron. To conclude, we prove
that for special values of the parameters of the system the neurons become
perfectly correlated, a phenomenon that we have called stochastic
synchronization. These discoveries clearly prevent the use of the mean-field
theory in the description of the neural network.