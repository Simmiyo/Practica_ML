Unsupervised learning by a nonlinear network with Hebbian excitatory and
  anti-Hebbian inhibitory neurons
This paper introduces a rate-based nonlinear neural network in which
excitatory (E) neurons receive feedforward excitation from sensory (S) neurons,
and inhibit each other through disynaptic pathways mediated by inhibitory (I)
interneurons. Correlation-based plasticity of disynaptic inhibition serves to
incompletely decorrelate E neuron activity, pushing the E neurons to learn
distinct sensory features. The plasticity equations additionally contain
"extra" terms fostering competition between excitatory synapses converging onto
the same postsynaptic neuron and inhibitory synapses diverging from the same
presynaptic neuron. The parameters of competition between S$\to$E connections
can be adjusted to make learned features look more like "parts" or "wholes."
The parameters of competition between I-E connections can be adjusted to set
the typical decorrelatedness and sparsity of E neuron activity. Numerical
simulations of unsupervised learning show that relatively few I neurons can be
sufficient for achieving good decorrelation, and increasing the number of I
neurons makes decorrelation more complete. Excitatory and inhibitory inputs to
active E neurons are approximately balanced as a result of learning.