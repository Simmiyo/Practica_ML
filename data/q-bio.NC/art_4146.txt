A primer on information theory, with applications to neuroscience
Given the constant rise in quantity and quality of data obtained from neural
systems on many scales ranging from molecular to systems',
information-theoretic analyses became increasingly necessary during the past
few decades in the neurosciences. Such analyses can provide deep insights into
the functionality of such systems, as well as a rigid mathematical theory and
quantitative measures of information processing in both healthy and diseased
states of neural systems. This chapter will present a short introduction to the
fundamentals of information theory, especially suited for people having a less
firm background in mathematics and probability theory. To begin, the
fundamentals of probability theory such as the notion of probability,
probability distributions, and random variables will be reviewed. Then, the
concepts of information and entropy (in the sense of Shannon), mutual
information, and transfer entropy (sometimes also referred to as conditional
mutual information) will be outlined. As these quantities cannot be computed
exactly from measured data in practice, estimation techniques for
information-theoretic quantities will be presented. The chapter will conclude
with the applications of information theory in the field of neuroscience,
including questions of possible medical applications and a short review of
software packages that can be used for information-theoretic analyses of neural
data.