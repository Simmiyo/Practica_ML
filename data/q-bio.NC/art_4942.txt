Deeper Image Quality Transfer: Training Low-Memory Neural Networks for
  3D Images
In this paper we address the memory demands that come with the processing of
3-dimensional, high-resolution, multi-channeled medical images in deep
learning. We exploit memory-efficient backpropagation techniques, to reduce the
memory complexity of network training from being linear in the network's depth,
to being roughly constant $ - $ permitting us to elongate deep architectures
with negligible memory increase. We evaluate our methodology in the paradigm of
Image Quality Transfer, whilst noting its potential application to various
tasks that use deep learning. We study the impact of depth on accuracy and show
that deeper models have more predictive power, which may exploit larger
training sets. We obtain substantially better results than the previous
state-of-the-art model with a slight memory increase, reducing the
root-mean-squared-error by $ 13\% $. Our code is publicly available.