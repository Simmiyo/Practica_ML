Do Brain Networks Evolve by Maximizing their Information Flow Capacity?
We propose a working hypothesis supported by numerical simulations that brain
networks evolve based on the principle of the maximization of their internal
information flow capacity. We find that synchronous behavior and capacity of
information flow of the evolved networks reproduce well the same behaviors
observed in the brain dynamical networks of Caenorhabditis elegans and humans,
networks of Hindmarsh-Rose neurons with graphs given by these brain networks.
We make a strong case to verify our hypothesis by showing that the neural
networks with the closest graph distance to the brain networks of
Caenorhabditis elegans and humans are the Hindmarsh-Rose neural networks
evolved with coupling strengths that maximize information flow capacity.
Surprisingly, we find that global neural synchronization levels decrease during
brain evolution, reflecting on an underlying global no Hebbian-like evolution
process, which is driven by no Hebbian-like learning behaviors for some of the
clusters during evolution, and Hebbian-like learning rules for clusters where
neurons increase their synchronization.