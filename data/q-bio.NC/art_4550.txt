Enhancing neural-network performance via assortativity
The performance of attractor neural networks has been shown to depend
crucially on the heterogeneity of the underlying topology. We take this
analysis a step further by examining the effect of degree-degree correlations
-- or assortativity -- on neural-network behavior. We make use of a method
recently put forward for studying correlated networks and dynamics thereon,
both analytically and computationally, which is independent of how the topology
may have evolved. We show how the robustness to noise is greatly enhanced in
assortative (positively correlated) neural networks, especially if it is the
hub neurons that store the information.