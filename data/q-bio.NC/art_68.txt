Gradient learning in spiking neural networks by dynamic perturbation of
  conductances
We present a method of estimating the gradient of an objective function with
respect to the synaptic weights of a spiking neural network. The method works
by measuring the fluctuations in the objective function in response to dynamic
perturbation of the membrane conductances of the neurons. It is compatible with
recurrent networks of conductance-based model neurons with dynamic synapses.
The method can be interpreted as a biologically plausible synaptic learning
rule, if the dynamic perturbations are generated by a special class of
``empiric'' synapses driven by random spike trains from an external source.