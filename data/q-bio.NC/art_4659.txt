Towards deep learning with spiking neurons in energy based models with
  contrastive Hebbian plasticity
In machine learning, error back-propagation in multi-layer neural networks
(deep learning) has been impressively successful in supervised and
reinforcement learning tasks. As a model for learning in the brain, however,
deep learning has long been regarded as implausible, since it relies in its
basic form on a non-local plasticity rule. To overcome this problem,
energy-based models with local contrastive Hebbian learning were proposed and
tested on a classification task with networks of rate neurons. We extended this
work by implementing and testing such a model with networks of leaky
integrate-and-fire neurons. Preliminary results indicate that it is possible to
learn a non-linear regression task with hidden layers, spiking neurons and a
local synaptic plasticity rule.