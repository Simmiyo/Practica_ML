Artificial Neurons with Arbitrarily Complex Internal Structures
Artificial neurons with arbitrarily complex internal structure are
introduced. The neurons can be described in terms of a set of internal
variables, a set activation functions which describe the time evolution of
these variables and a set of characteristic functions which control how the
neurons interact with one another. The information capacity of attractor
networks composed of these generalized neurons is shown to reach the maximum
allowed bound. A simple example taken from the domain of pattern recognition
demonstrates the increased computational power of these neurons. Furthermore, a
specific class of generalized neurons gives rise to a simple transformation
relating attractor networks of generalized neurons to standard three layer
feed-forward networks. Given this correspondence, we conjecture that the
maximum information capacity of a three layer feed-forward network is 2 bits
per weight.