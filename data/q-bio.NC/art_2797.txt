Bayesian Modeling of Motion Perception using Dynamical Stochastic
  Textures
A common practice to account for psychophysical biases in vision is to frame
them as consequences of a dynamic process relying on optimal inference with
respect to a generative model. The present study details the complete
formulation of such a generative model intended to probe visual motion
perception with a dynamic texture model. It is first derived in a set of
axiomatic steps constrained by biological plausibility. We extend previous
contributions by detailing three equivalent formulations of this texture model.
First, the composite dynamic textures are constructed by the random aggregation
of warped patterns, which can be viewed as 3D Gaussian fields. Secondly, these
textures are cast as solutions to a stochastic partial differential equation
(sPDE). This essential step enables real time, on-the-fly texture synthesis
using time-discretized auto-regressive processes. It also allows for the
derivation of a local motion-energy model, which corresponds to the
log-likelihood of the probability density. The log-likelihoods are essential
for the construction of a Bayesian inference framework. We use the dynamic
texture model to psychophysically probe speed perception in humans using
zoom-like changes in the spatial frequency content of the stimulus. The human
data replicates previous findings showing perceived speed to be positively
biased by spatial frequency increments. A Bayesian observer who combines a
Gaussian likelihood centered at the true speed and a spatial frequency
dependent width with a "slow speed prior" successfully accounts for the
perceptual bias. More precisely, the bias arises from a decrease in the
observer's likelihood width estimated from the experiments as the spatial
frequency increases. Such a trend is compatible with the trend of the dynamic
texture likelihood width.